{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hand Detection using CNN\n![Hand Detection](https://user-images.githubusercontent.com/50156227/166168918-a5caff72-e068-41ac-b1cb-91b79a8a1311.gif)\n<br>\n<h4><b>Model is ready to be used for apps & APIs, (check the last cell for 'How to use')</b></h4>","metadata":{}},{"cell_type":"code","source":"import cv2\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageDraw\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.layers import Input, Dense, Add, Conv2D, SeparableConv2D\nfrom tensorflow.keras.layers import BatchNormalization, AveragePooling2D\nfrom tensorflow.keras.layers import LeakyReLU, MaxPooling2D, Flatten\nfrom tensorflow.keras.models import Model\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K","metadata":{"papermill":{"duration":6.567837,"end_time":"2022-02-13T08:44:02.64692","exception":false,"start_time":"2022-02-13T08:43:56.079083","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-30T19:57:00.301134Z","iopub.execute_input":"2022-04-30T19:57:00.301422Z","iopub.status.idle":"2022-04-30T19:57:06.241699Z","shell.execute_reply.started":"2022-04-30T19:57:00.301395Z","shell.execute_reply":"2022-04-30T19:57:06.240974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_HEIGHT = 416\nIMG_WIDTH = 416\nX_FACTOR = IMG_WIDTH / 600 # 600 is dataset images shape\nY_FACTOR = IMG_HEIGHT / 600\nBATCH_SIZE = 128\nEPOCHS = 35\nlearning_rate = 0.0032\ndataset_path = '../input/hand-detection-dataset-factory/hands'","metadata":{"execution":{"iopub.status.busy":"2022-04-30T19:57:06.243344Z","iopub.execute_input":"2022-04-30T19:57:06.243604Z","iopub.status.idle":"2022-04-30T19:57:06.248313Z","shell.execute_reply.started":"2022-04-30T19:57:06.243569Z","shell.execute_reply":"2022-04-30T19:57:06.247651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def grabPaths(filepath):\n    labels = [str(filepath[i]).split(\"/\")[-1] \\\n              for i in range(len(filepath))]\n    filepath = pd.Series(filepath, name='path').astype(str)\n    df = pd.DataFrame(filepath)    \n    return df\n\ndef rescale_boxes(boxes):\n    # Rescale boxes (since we resscaled images sizes)\n    boxes = np.array(boxes).astype(np.float32)\n    boxes[:,[0,2]] = boxes[:,[0,2]] * X_FACTOR\n    boxes[:,[1,3]] = boxes[:,[1,3]] * Y_FACTOR\n    return boxes\n\ndef inverse_rescale_boxes(boxes):\n    # Inverse scale of box coordinates\n    # You should inverse scale based on the image\n    boxes[:,[0,2]] = boxes[:,[0,2]] / X_FACTOR\n    boxes[:,[1,3]] = boxes[:,[1,3]] / Y_FACTOR\n    return boxes\n\n\ndef plot_bbox(image, yt_box, yp_box=None, norm=False):\n    # Given an image and box coordinates, draw the box on the image\n    if norm:\n        image = image * 255.\n        image = image.astype(\"uint8\")\n    \n    try:\n        pil_img = Image.fromarray(image)\n    except:\n        pil_img = Image.fromarray(image.astype('uint8'))\n        \n    draw_img = ImageDraw.Draw(pil_img)\n    \n    x1, y1, w, h = yt_box\n    x2, y2 = x1+w, y1+h\n    draw_img.rectangle((x1, y1, x2, y2), outline='green')\n    \n    if yp_box is not None:\n        x1, y1, w, h = yp_box\n        x2, y2 = x1+w, y1+h\n        draw_img.rectangle((x1, y1, x2, y2), outline='red')\n    return pil_img\n\n\ndef convblock(previous_layer, n_filters, filter_windows=(3,3,3), padding='same', pool=None):\n    # [DepthwiseConv -> BatchNorm -> LeakyReLU] x3 + Residual connection -> Pooling (optional)\n\n    x = SeparableConv2D(n_filters[0], filter_windows[0], padding=padding)(previous_layer)\n    x_short = x\n    x = BatchNormalization()(x)\n    x = LeakyReLU()(x)\n\n    x = SeparableConv2D(n_filters[1], filter_windows[1], padding=padding)(x)\n    x = BatchNormalization()(x)\n    x = LeakyReLU()(x)\n\n    x = SeparableConv2D(n_filters[2], filter_windows[2], padding=padding)(x)\n    x = BatchNormalization()(x)\n    x = Add()([x, x_short])\n    x = LeakyReLU()(x)\n    \n    if pool == 'max':\n        x = MaxPooling2D(pool_size=(2,2))(x)\n    elif pool == 'avg':\n        x = AveragePooling2D(pool_size=(2,2))(x)\n    \n    return x\n\n\n# Function to calculate MSE Loss function\n# for samples where object exists\ndef custom_mse(y_true, y_pred):\n    mask = K.not_equal(K.sum(y_true, axis=1), 0.0)\n    y_true_custom = y_true[mask]\n    y_pred_custom = y_pred[mask]\n    mse = tf.keras.losses.MeanSquaredError()\n    result = mse(y_true_custom, y_pred_custom) # * 0.3\n    return result\n\n\n# Function to preview samples of the dataset\ndef visualize_samples(datagen, row_col_len=4, figsize=None):\n    figsize = figsize or np.array((row_col_len, row_col_len)) * 4\n    fig, ax = plt.subplots(row_col_len, row_col_len, figsize=figsize)\n    for i in range(row_col_len):\n        for j in range(row_col_len):\n            batch_index = np.random.randint(0, BATCH_SIZE/2)\n            output_classes = np.array(datagen[batch_index][1]['class_out'])\n            classes_true = np.where(output_classes == 1)[0]\n            sample_index = classes_true[np.random.randint(0, classes_true.shape[0])]\n            image = datagen[batch_index][0][sample_index]\n            box = datagen[batch_index][1]['box_out'][sample_index]\n            plotted_box = plot_bbox(image, box, norm=True)\n            ax[i,j].imshow(plotted_box)\n            ax[i,j].set_axis_off()\n    plt.show()\n    \n\n# Function to make a prediction during training\ndef visualize_prediction(model, data):    \n    # Select a sample where an object exists\n    output_classes = np.array(data[0][1]['class_out'])\n    sample_index = np.where(output_classes == 1)[0]\n    sample_index = sample_index[0] if sample_index[0] else 0 \n    \n    # Get image\n    image = np.array([data[0][0][sample_index]])\n    \n    # Set y_true & y_pred for class & bounding box\n    yt_box = np.array([data[0][1]['box_out'][sample_index]])\n    yt_class = np.array([data[0][1]['class_out'][sample_index]])\n    yp_class, yp_box = model.predict(image)\n\n    # Plot bounding box on image & show it\n    image_plotted = plot_bbox(image[0], yt_box[0], yp_box[0], norm=True) \n    plt.imshow(image_plotted)\n    plt.axis('off')\n    \n    # Print y_true class & y_pred class\n    print(\"Class: y_true=\", yt_class, \" | y_pred=\", int(yp_class >= 0.5))\n    plt.show()","metadata":{"papermill":{"duration":0.029573,"end_time":"2022-02-13T08:44:02.688637","exception":false,"start_time":"2022-02-13T08:44:02.659064","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-30T20:14:34.220831Z","iopub.execute_input":"2022-04-30T20:14:34.221086Z","iopub.status.idle":"2022-04-30T20:14:34.420734Z","shell.execute_reply.started":"2022-04-30T20:14:34.221055Z","shell.execute_reply":"2022-04-30T20:14:34.419965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read CSV\ndataset = pd.read_csv('../input/hand-detection-dataset-factory/dataset.csv')\n\n# REMOVE THE LINE BELOW TO USE ALL SAMPLES\n# dataset = dataset.sample(n=15000)\n\n\ndataset.columns = ['path', 'object_exists', 'x', 'y', 'w', 'h']\n\n\n# List directories of files\ntrain_image_dir_hand = Path(dataset_path)\ntrain_filepaths_hand = list(train_image_dir_hand.glob(r'**/*.png'))\n\n# Create dataframe of {paths, labels}\ntrain_df_hand = grabPaths(train_filepaths_hand)\ndataset['path'] = \"../input/hand-detection-dataset-factory/hands/\" + dataset['path']\n\n# Resize boxes\ndataset.iloc[:,2:] = rescale_boxes(dataset.iloc[:,2:])","metadata":{"papermill":{"duration":18.635473,"end_time":"2022-02-13T08:44:21.335769","exception":false,"start_time":"2022-02-13T08:44:02.700296","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-30T20:02:38.501309Z","iopub.execute_input":"2022-04-30T20:02:38.501568Z","iopub.status.idle":"2022-04-30T20:02:49.258395Z","shell.execute_reply.started":"2022-04-30T20:02:38.501539Z","shell.execute_reply":"2022-04-30T20:02:49.257629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, test_df = train_test_split(dataset, test_size=0.2)","metadata":{"papermill":{"duration":0.02937,"end_time":"2022-02-13T08:44:21.376494","exception":false,"start_time":"2022-02-13T08:44:21.347124","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-30T20:02:49.260102Z","iopub.execute_input":"2022-04-30T20:02:49.26035Z","iopub.status.idle":"2022-04-30T20:02:49.270654Z","shell.execute_reply.started":"2022-04-30T20:02:49.260315Z","shell.execute_reply":"2022-04-30T20:02:49.269762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiOutputGen(tf.keras.utils.Sequence):\n    def __init__(self, input_gen, output_gen):\n        self.inpgen = input_gen\n        self.outgen = output_gen\n\n    def __len__(self):\n        return len(self.inpgen)\n\n    def __getitem__(self, i):\n        images = self.inpgen[i]\n        start = i * images.shape[0]\n        end = (i+1) * images.shape[0]\n        classes_num = self.outgen.iloc[start:end,0].values\n        x = self.outgen.iloc[start:end,1]\n        y = self.outgen.iloc[start:end,2]\n        w = self.outgen.iloc[start:end,3]\n        h = self.outgen.iloc[start:end,4]\n#         return images, np.array([x, y, w, h]).T\n        return images, {'class_out':classes_num, 'box_out':np.array([x, y, w, h]).T}\n\n    def on_epoch_end(self):\n        self.inpgen.on_epoch_end()\n    \n# Class to visualize predictions during training\nclass VisualOutput(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        global custom_test_gen\n        visualize_prediction(self.model, custom_test_gen)","metadata":{"papermill":{"duration":0.022405,"end_time":"2022-02-13T08:44:21.411579","exception":false,"start_time":"2022-02-13T08:44:21.389174","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-30T20:29:58.412045Z","iopub.execute_input":"2022-04-30T20:29:58.412342Z","iopub.status.idle":"2022-04-30T20:29:58.421444Z","shell.execute_reply.started":"2022-04-30T20:29:58.412313Z","shell.execute_reply":"2022-04-30T20:29:58.420777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    brightness_range=(0.8, 1.2),\n    rescale = 1./255.,\n)\n\ntest_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale = 1./255.,\n)\n\ntrain_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='path',\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    color_mode='rgb',\n    class_mode=None,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n)\n\ntest_images = test_generator.flow_from_dataframe(\n    dataframe=test_df,\n    x_col='path',\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    color_mode='rgb',\n    class_mode=None,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n)\n\n\ncustom_train_gen = MultiOutputGen(train_images, train_df.iloc[:,1:])\ncustom_test_gen = MultiOutputGen(test_images, test_df.iloc[:,1:])","metadata":{"papermill":{"duration":6.889778,"end_time":"2022-02-13T08:44:28.3126","exception":false,"start_time":"2022-02-13T08:44:21.422822","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-30T20:03:56.159273Z","iopub.execute_input":"2022-04-30T20:03:56.15955Z","iopub.status.idle":"2022-04-30T20:04:01.236883Z","shell.execute_reply.started":"2022-04-30T20:03:56.159513Z","shell.execute_reply":"2022-04-30T20:04:01.236061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_samples(custom_train_gen)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T20:14:41.59521Z","iopub.execute_input":"2022-04-30T20:14:41.595913Z","iopub.status.idle":"2022-04-30T20:16:50.65784Z","shell.execute_reply.started":"2022-04-30T20:14:41.595872Z","shell.execute_reply":"2022-04-30T20:16:50.657198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BEST MODEL\n\ninp = Input(shape=(IMG_HEIGHT,IMG_WIDTH,3), name='image')\n\nX = SeparableConv2D(64, (7,7), strides=2, padding='valid')(inp)\nX = MaxPooling2D(pool_size=(2,2), strides=2)(X)\n\nX = SeparableConv2D(192, (3,3), strides=1, padding='same')(X)\nX = MaxPooling2D(pool_size=(2,2), strides=2)(X)\n\nX = convblock(X, [16, 16, 16], pool='max')\nX = convblock(X, [32, 32, 32], pool='max')\nX = convblock(X, [64, 64, 64], pool='max')\nX = convblock(X, [128, 128, 128], pool='max')\nX = convblock(X, [256, 256, 256])\n\nX = Flatten()(X)\n\nXbox = Dense(1024)(X)\nXbox = LeakyReLU()(Xbox)\n\nXhand = Dense(512)(X)\nXhand = LeakyReLU()(Xhand)\n\n\nbox_output = Dense(4, name='box_out')(Xbox)\nclass_output = Dense(1, activation='sigmoid', name='class_out')(Xhand)\n\nmodel = Model(inp, [class_output, box_output])\nmodel.summary()","metadata":{"papermill":{"duration":2.800013,"end_time":"2022-02-13T08:44:31.164","exception":false,"start_time":"2022-02-13T08:44:28.363987","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-30T20:16:50.659301Z","iopub.execute_input":"2022-04-30T20:16:50.659643Z","iopub.status.idle":"2022-04-30T20:16:53.536182Z","shell.execute_reply.started":"2022-04-30T20:16:50.65961Z","shell.execute_reply":"2022-04-30T20:16:53.535334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adam = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n\nsave_best_model = tf.keras.callbacks.ModelCheckpoint(\n    filepath='./best_hand_detection.h5',\n    monitor='val_loss',\n    mode='min',\n    save_best_only=True)\n\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_box_out_loss\",\n    factor=tf.math.exp(-0.15),\n    patience=2,\n    verbose=1,\n    mode=\"min\",\n    min_lr=0.0004,\n)\n\nmodel.compile(\n    loss={'box_out':custom_mse, 'class_out':'binary_crossentropy'},\n    optimizer=adam,\n    metrics={'class_out':'accuracy'}\n)","metadata":{"papermill":{"duration":0.03169,"end_time":"2022-02-13T08:44:31.209795","exception":false,"start_time":"2022-02-13T08:44:31.178105","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-30T22:34:25.275598Z","iopub.execute_input":"2022-04-30T22:34:25.276289Z","iopub.status.idle":"2022-04-30T22:34:25.299494Z","shell.execute_reply.started":"2022-04-30T22:34:25.276251Z","shell.execute_reply":"2022-04-30T22:34:25.298858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    custom_train_gen,\n    epochs=EPOCHS,\n    validation_data=custom_test_gen,\n    callbacks=[\n        reduce_lr,\n        VisualOutput(),\n        save_best_model\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:01:07.089074Z","iopub.execute_input":"2022-05-01T02:01:07.089358Z","iopub.status.idle":"2022-05-01T02:17:04.728829Z","shell.execute_reply.started":"2022-05-01T02:01:07.08933Z","shell.execute_reply":"2022-05-01T02:17:04.727945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss Graph\n\n%matplotlib inline\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(loss))\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss (MSE)')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-19T18:10:45.544429Z","iopub.execute_input":"2022-02-19T18:10:45.5455Z","iopub.status.idle":"2022-02-19T18:10:45.57554Z","shell.execute_reply.started":"2022-02-19T18:10:45.545453Z","shell.execute_reply":"2022-02-19T18:10:45.572197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save model for later use\n\n# for python apps (& APIs)\nmodel.save(\"Hand Detection.h5\")\n\n# for mobile apps & microcontrollers (TensorFlow Lite)\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\nwith open(\"Hand Detection Lite.tflite\", 'wb') as file:\n    file.write(tflite_model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# How to use the model with Camera (and as API)\n\n1- For APIs, Python Apps\nDownload 'Hand Detection.h5'.\n\n2- For Non-Python Apps (i.e. mobile, embedded)\nDownload Lite version for mobile apps\nand search for some package to use Tflite (package:tflite for Flutter & Java)\n\nRun the following code in the same folder with the model","metadata":{}},{"cell_type":"code","source":"# import cv2\n# import tensorflow as tf\n# from tensorflow.keras import backend as K\n# from tensorflow.keras.models import load_model\n\n# # Function to calculate MSE Loss function\n# # for samples where object exists\n# def custom_mse(y_true, y_pred):\n#     mask = K.not_equal(K.sum(y_true, axis=1), 0.0)\n#     y_true_custom = y_true[mask]\n#     y_pred_custom = y_pred[mask]\n#     mse = tf.keras.losses.MeanSquaredError()\n#     result = mse(y_true_custom, y_pred_custom) # * 0.3\n#     return result\n\n# model = load_model(\"Hand Detection.h5\", custom_objects={'custom_mse':custom_mse})\n\n# IMG_HEIGHT = 416\n# IMG_WIDTH = 416\n\n# cam = cv2.VideoCapture(0)\n# while True:\n#     ret, frame = cam.read()\n#     if ret:\n        \n#         # How to use as API (given img:frame as input)\n#         image = cv2.resize(frame, (IMG_WIDTH, IMG_HEIGHT))\n#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n#         image = image / 255.\n#         yp_class, yp_box = model.predict(np.array([image]))\n#         x, y, w, h= yp_box[0]        \n#         x, w = round(IMG_WIDTH / (IMG_WIDTH / frame.shape[1]) ), round(IMG_WIDTH / (IMG_WIDTH / frame.shape[1]))\n#         y, h = round(IMG_HEIGHT / (IMG_HEIGHT / frame.shape[0])), round(h / (IMG_HEIGHT / frame.shape[0])) # / 560\n#         # END: how to use as API\n        \n#         if yp_class[0] >= 0.5:\n#              frame= cv2.rectangle(frame, (x, y), (x+w, y+h), thickness=2, color=(0,255,0))\n#         cv2.imshow(\"mywindow\", frame)\n#     if cv2.waitKey(1) == ord('q'):\n#         break\n# cam.release()\n# cv2.destroyAllWindows()","metadata":{"papermill":{"duration":1.750757,"end_time":"2022-02-13T12:17:20.180184","exception":false,"start_time":"2022-02-13T12:17:18.429427","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}